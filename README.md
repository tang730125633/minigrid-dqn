# MiniGrid DQN: Reinforcement Learning with Reward Shaping

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A Deep Q-Network (DQN) agent for the MiniGrid-Empty-8x8-v0 environment, with Potential-Based Reward Shaping (PBRS) improvement and ablation studies.

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰

### Windows ç”¨æˆ·
ğŸ‘‰ æŸ¥çœ‹ [**Windows å¿«é€Ÿå¼€å§‹æŒ‡å—**](README_WINDOWS.md)

1. ä¸‹è½½é¡¹ç›®å¹¶è§£å‹åˆ°æ¡Œé¢
2. **åŒå‡» `å®‰è£…å¹¶å¯åŠ¨.bat`**
3. æµè§ˆå™¨è‡ªåŠ¨æ‰“å¼€å¯è§†åŒ–ç•Œé¢

### macOS / Linux ç”¨æˆ·
```bash
git clone <repository-url>
cd minigrid-dqn
bash å®‰è£…å¹¶å¯åŠ¨.sh
```

## ğŸŒ Web å¯è§†åŒ–ç•Œé¢

æœ¬é¡¹ç›®åŒ…å«ä¸€ä¸ª **Streamlit Web ç•Œé¢**ï¼Œæ— éœ€ç¼–ç¨‹å³å¯ï¼š
- ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿å’ŒæˆåŠŸç‡å¯¹æ¯”
- ğŸ¬ è§‚çœ‹æ™ºèƒ½ä½“å­¦ä¹ è¿‡ç¨‹çš„åŠ¨ç”»
- âš™ï¸ ä¸€é”®è¿è¡Œæ–°å®éªŒ
- ğŸ“ˆ å¯¹æ¯”ä¸åŒç®—æ³•çš„æ€§èƒ½

å¯åŠ¨åè®¿é—® http://localhost:8501

## Project Structure

```
minigrid-dqn/
â”œâ”€â”€ configs/                 # Experiment configurations (YAML)
â”‚   â”œâ”€â”€ default.yaml         # Baseline DQN
â”‚   â”œâ”€â”€ reward_shaping.yaml  # DQN + PBRS
â”‚   â”œâ”€â”€ ablation_gamma.yaml  # Ablation: gamma values
â”‚   â””â”€â”€ ablation_no_target.yaml  # Ablation: no target network
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ config.py            # Configuration loading
â”‚   â”œâ”€â”€ env_utils.py         # Environment wrappers (RewardShaping, ObsPreprocess)
â”‚   â”œâ”€â”€ network.py           # CNN Q-Network
â”‚   â”œâ”€â”€ replay_buffer.py     # Experience replay buffer
â”‚   â”œâ”€â”€ dqn_agent.py         # DQN agent
â”‚   â”œâ”€â”€ train.py             # Training loop
â”‚   â”œâ”€â”€ evaluate.py          # Evaluation script
â”‚   â””â”€â”€ visualize.py         # Chart and GIF generation
â”œâ”€â”€ scripts/                 # Automation scripts (.sh + .bat)
â”œâ”€â”€ results/                 # Training results (auto-generated)
â”œâ”€â”€ logs/                    # TensorBoard logs (auto-generated)
â”œâ”€â”€ figures/                 # Comparison charts (auto-generated)
â””â”€â”€ gifs/                    # Demo GIFs (auto-generated)
```

## Requirements

- Python 3.8+
- PyTorch >= 2.0
- CPU only (no GPU required)

## Setup

### Windows

```bash
# 1. Create virtual environment
python -m venv venv
venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt
```

### macOS / Linux

```bash
# 1. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt
```

## Quick Start

### Run all experiments (recommended)

**Windows:**
```bash
scripts\train_all.bat
scripts\evaluate_all.bat
scripts\generate_report.bat
```

**macOS / Linux:**
```bash
bash scripts/train_all.sh
bash scripts/evaluate_all.sh
bash scripts/generate_report.sh
```

### Run a single experiment

```bash
# Baseline DQN
python -m src.train --config configs/default.yaml --seed 0

# DQN + Reward Shaping
python -m src.train --config configs/reward_shaping.yaml --seed 0

# Ablation: different gamma
python -m src.train --config configs/ablation_gamma.yaml --seed 0 --gamma 0.9

# Ablation: no target network
python -m src.train --config configs/ablation_no_target.yaml --seed 0
```

### Evaluate trained models

```bash
python -m src.evaluate --results_dir results --num_episodes 100
```

### Generate charts and GIFs

```bash
python -m src.visualize
```

### Monitor training with TensorBoard

```bash
tensorboard --logdir logs
```

## Method

### Baseline: DQN

Standard Deep Q-Network with:
- CNN architecture for 7x7x3 MiniGrid observations
- Experience replay (buffer size: 100k)
- Target network (updated every 1000 steps)
- Linear epsilon decay (1.0 â†’ 0.01 over 20k steps)

### Improvement: Potential-Based Reward Shaping (PBRS)

Adds a shaping reward based on Manhattan distance to goal:

```
r_shaped = r_original + gamma * Phi(s') - Phi(s)
Phi(s) = 1 - (manhattan_distance_to_goal / max_distance)
```

This guides exploration toward the goal while preserving optimal policy (Ng et al., 1999).

### Ablation Studies

1. **Gamma sensitivity**: gamma âˆˆ {0.9, 0.99, 0.999} â€” tests how discount factor affects PBRS
2. **Target network**: with vs without â€” demonstrates DQN stabilization

Main comparison: 3 seeds (0, 1, 2), reporting mean Â± std. Ablation studies: 1 seed.

## Results

### Main Comparison (3 seeds)

| Method | Seed 0 | Seed 1 | Seed 2 | Mean Â± Std |
|--------|--------|--------|--------|------------|
| Baseline DQN | 0% | 100% | 100% | 66.7% Â± 47.1% |
| **DQN + PBRS** | **100%** | **100%** | **100%** | **100% Â± 0%** |

### Ablation: Gamma

| Gamma | Success Rate |
|-------|-------------|
| 0.9 | 100% |
| 0.99 | 100% |
| 0.999 | 100% |

### Ablation: Target Network

| Configuration | Success Rate |
|---------------|-------------|
| With target network | 100% |
| Without target network | 0% |

## Key Hyperparameters

| Parameter | Value |
|-----------|-------|
| Learning rate | 1e-4 |
| Gamma | 0.99 |
| Batch size | 64 |
| Buffer size | 100,000 |
| Epsilon decay | 20,000 steps |
| Target update | Every 1,000 steps |
| Training episodes | 3,000 |
| Evaluation episodes | 100 |

## References

- Mnih et al., 2015. "Human-level control through deep reinforcement learning." Nature.
- Ng et al., 1999. "Policy invariance under reward transformations." ICML.
- Chevalier-Boisvert et al., 2023. "Minigrid & Miniworld: Modular & Customizable RL Environments." NeurIPS.
